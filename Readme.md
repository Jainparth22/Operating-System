2024-12-27 17:46
Status:
Tags:
Hash-Tags:


# OS 27122024


---

# **Important topics for OS:**

---

#### **CPU and Process Management**

1. **CPU scheduling**
2. **Scheduling algorithms**
3. **Scheduling problems**
4. **Semaphore, mutual exclusion, critical section**
    - Number of processes
    - Basic sections
5. **Scheduler and Dispatcher**
    - Scheduler: Types and differences
    - Dispatcher: Responsibility and fundamentals
6. **Parent process**
    - Zombie process, orphan process
    - Signals in parent processes, fork(), necessity of wait signal
7. **Deadlocks**
    - Concepts of prevention and avoidance
8. **Multi-processors**
    - Functionality, symmetric and asymmetric

---

#### **Memory Management**

1. **Memory**
    - Logical and physical memory
    - Numerical problems
2. **Paging**
    - Purpose and types of fragmentation: Internal and external
    - Mechanisms to overcome external fragmentation
3. **Effective memory access time**
    - Calculation and problem-solving
4. **Page table and page memory**
5. **Paging and segmentation fragmentations**
6. **Contiguous memory allocation**
7. **Linked memory allocation**

---

#### **System Calls and Kernel**

1. **System calls**
    - Fundamental functionalities, why they are needed
2. **Monitor calls**
    - Purpose and significance
3. **Kernel**
    - Functionality and purpose
    - Differences between kernel and user operations
    - Kernel-level vs. user-level processes
    - Cost: User vs. kernel processes

---

#### **Disk Management**

1. **Disk scheduling**
    - Concepts and problems
    - Types (three types)

---

#### **Interfaces**

1. **Types of interfaces**
    - GUI vs. Command-line interfaces: Differences
    - Application perspective

---

#### **Bootstrap Loader**

1. **Bootstrap loader (Firmware)**
    - Why it is needed
    - Role during system loading
    - Collection of bits

---

#### **Other Important Topics**

1. **Access matrix**
    - Definition, representation, and significance
2. **Device drivers and controllers**
    - Definitions, functionalities, and importance

---

### **Numerical Problems**

1. **CPU Scheduling Algorithms**
    - FCFS, SJF, Round Robin, and Priority Scheduling
    - Related numerical problems
2. **Effective Memory Access Time**
    - Calculation and problem-solving
3. **Disk Scheduling**
    - Problems involving FCFS, SSTF, SCAN, and C-SCAN algorithms
4. **Page Replacement Algorithms**
    - FIFO (First-In-First-Out)
    - LRU (Least Recently Used)

---

Okay, let's delve into the details of CPU and process management, focusing on CPU scheduling, scheduling algorithms, and related problems, drawing from the provided sources and our conversation history.

**I. CPU Scheduling: Core Concepts**

- **Operating System as a Resource Manager:** An operating system is a control program that manages the execution of user programs, aiming to prevent errors and the improper use of a computer. It acts like a government, providing the environment for other programs to do useful work.
- **Multiprogramming:** To enhance CPU utilization and system response, several processes are kept in memory concurrently. This approach means that the CPU switches between multiple jobs to make sure it is always doing work.
- **CPU-I/O Burst Cycle:** Processes alternate between CPU bursts, where the CPU is actively executing instructions, and I/O bursts, where the process waits for an I/O operation. I/O-bound processes have many short CPU bursts, while CPU-bound processes have few long bursts.
- **CPU Scheduler:** The short-term scheduler (also called the CPU scheduler) selects a process from the ready queue for execution. The ready queue holds processes in main memory that are ready and waiting to execute.
- **Dispatcher:** The dispatcher module hands control of the CPU to the process chosen by the scheduler, performing context switching, switching to user mode, and jumping to the correct location in the user program. **Dispatch latency** is the time it takes to stop one process and start another.
- **Goals of Scheduling:**
    - Maximize **CPU utilization** by keeping the CPU as busy as possible. Ideally, utilization should range from 40% (lightly loaded) to 90% (heavily loaded).
    - Increase **throughput**, which is the number of processes completed per time unit. This can range from one process per hour for long processes to ten processes per second for short transactions.
- **Process States:** As a process executes, it goes through various states, such as running, waiting, and ready.

**II. Scheduling Algorithms**

- **First-Come, First-Served (FCFS):**
    - Processes are assigned the CPU in the order they request it.
    - Simple to implement with a FIFO queue.
    - **Non-preemptive:** Once a process has the CPU, it retains it until it completes or requires I/O.
    - Prone to the **convoy effect,** where short processes wait behind a long process, reducing efficiency.
    - Not ideal for time-sharing systems.
- **Shortest-Job-First (SJF):**
    - The CPU is given to the process with the smallest next CPU burst.
    - **Optimal** with the minimum average waiting time for a set of processes.
    - Difficult to implement, since it requires knowledge of the length of the next CPU burst, which is not always known.
    - Can be either **preemptive** or **non-preemptive**. Preemptive SJF is also known as **shortest-remaining-time-first** scheduling.
    - The length of the next CPU burst can be estimated using exponential averaging of previous bursts.
- **Priority Scheduling:**
    - Each process has a priority, with the CPU allocated to the highest priority process first.
    - Equal-priority processes are scheduled in FCFS order.
    - SJF is a special case of priority scheduling where the priority is based on the inverse of the next CPU burst.
    - Can be **preemptive** or **non-preemptive**.
    - Subject to **starvation**, where low-priority processes may be delayed indefinitely.
    - **Aging** can prevent starvation by increasing the priority of processes as they wait.
- **Round-Robin (RR):**
    - Designed for time-sharing systems.
    - Each process is given a small unit of CPU time called a **time quantum** or **time slice**.
    - The ready queue is treated as a circular queue.
    - **Preemptive:** If a process does not finish within its time quantum, it is moved to the back of the ready queue.
    - If the time quantum is too large, RR behaves like FCFS. If it's too small, excessive context switches occur. A good rule is that 80% of CPU bursts should be shorter than the time quantum.
- **Multilevel Queue Scheduling:**
    - The ready queue is split into multiple separate queues.
    - Processes are assigned to queues based on their characteristics, such as priority or type.
    - Each queue can have its own scheduling algorithm. For example, a foreground queue can use RR while a background queue uses FCFS.
    - Scheduling is required between the queues and can be done using fixed priority or time slicing.
- **Multilevel Feedback Queue Scheduling:**
    - Processes can move between different queues.
    - Processes using too much CPU time are moved to lower-priority queues.
    - Processes waiting too long in lower-priority queues can be moved to higher-priority queues (aging).
    - Aims to separate processes based on their CPU burst characteristics.

**III. Scheduling Problems**

- **Race Conditions:** Preemptive scheduling can cause race conditions when multiple processes concurrently access shared data.
- **Starvation:** With priority scheduling, low-priority processes might never get the CPU.
- **Convoy Effect:** In FCFS scheduling, long processes can delay shorter processes.
- **Indefinite Blocking:** A process may be ready to run but has to wait indefinitely for the CPU.
- **Data Inconsistency** Concurrent access to shared data may result in data inconsistency.

**IV. Key Concepts in CPU Scheduling**

- **Preemptive Scheduling:** The operating system can interrupt a running process and give the CPU to another process.
- **Non-preemptive Scheduling:** A process retains the CPU until it terminates or requests I/O.
- **Context Switching:** The process of saving the state of the current process and loading the state of the next process. This is an important part of the dispatcher's operation.
- **Time Quantum:** The specific amount of time that a process gets in the round-robin scheduling algorithm.
- **Throughput:** Measures the number of processes completed per time unit.
- **Waiting Time:** The amount of time a process spends in the ready queue.
- **Turnaround Time:** The time from when a process is submitted to when it is completed.
- **CPU Utilization:** The goal of keeping the CPU as busy as possible.

**V. Additional Insights**

- Scheduling is a fundamental aspect of operating system functionality.
- The choice of a scheduling algorithm depends on the system requirements.
- The ready queue is not always a simple FIFO queue.
- Operating systems must provide mechanisms for job synchronization and communication to ensure orderly execution.
- **System calls** provide the interface for accessing operating system services.

Okay, let's explore the concepts of semaphores, mutual exclusion, critical sections, process management, scheduling, and dispatching in detail, drawing from the sources.

**I. Semaphores**

- **Definition:** A semaphore is an integer variable used for process synchronization, accessed through two atomic operations: **wait()** and **signal()**. The wait() operation was originally termed P, and signal() was originally called V. - **P (Proberen)**: Also known as **wait()**, **decrement()**, or **down()**.
    
    - Decrements the semaphore value.
    - If the value becomes less than zero, the process is blocked (or put to sleep) until the resource is available.
- **V (Verhogen)**: Also known as **signal()**, **increment()**, or **up()**.
    
    - Increments the semaphore value.
    - If there are any processes waiting, one of them is unblocked.
    
- **Atomic Operations:** Modifications to the semaphore's integer value within wait() and signal() operations must be executed indivisibly. This ensures that when one process modifies the semaphore value, no other process can modify it simultaneously. Additionally, the testing of the integer value of S in wait(S) and its possible modification (S--) must be executed without interruption.
    
- **Types:**
    
    - **Counting semaphores** can have an unrestricted range of values, which are used to control access to a finite number of resources.
    - **Binary semaphores** can have values only between 0 and 1 and are similar to mutex locks in that they provide mutual exclusion. If a system does not provide mutex locks, binary semaphores can be used for mutual exclusion.
- **Usage:**
    
    - **Resource Control:** Counting semaphores are initialized to the number of available resources. A process performs a wait() operation to use a resource (decrementing the count) and a signal() operation to release a resource (incrementing the count).
    - **Synchronization:** Semaphores can also be used to synchronize the execution order of processes. For example, one process can signal another process after completing a task.
- **Implementation:**
    
    - **Busy Waiting Issue:** A basic implementation of wait() and signal() can lead to busy waiting, where a process continuously loops while waiting.
    - **Blocking Implementation:** To avoid busy waiting, a process can block itself when it executes a wait() operation and finds that the semaphore value is not positive. The blocked process is placed in a waiting queue associated with the semaphore, and its state is changed to waiting.
    - **Wakeup:** A blocked process is restarted using a wakeup() operation when another process executes a signal() operation. The process's state is changed to ready, and it is placed in the ready queue.
    - **Atomic Execution:** Semaphore operations must be executed atomically. In a single-processor system, this can be achieved by inhibiting interrupts. In a multiprocessor system, alternative locking techniques such as compare and swap() or spinlocks are used to ensure atomicity.
    - **Limited Busy Waiting:** The blocking implementation of semaphores moves busy waiting from the application's critical section to the critical section of the wait() and signal() operations. However, these sections are very short.

**II. Mutual Exclusion**

- **Definition:** Mutual exclusion is a critical requirement in concurrent systems where only one process can execute within its critical section at any given time.
- **Critical Section:** A critical section is a code segment where a process may be changing common variables, updating a table, writing a file, or performing other operations that require exclusive access.
- **Purpose:** Mutual exclusion prevents race conditions and ensures data consistency when multiple processes share resources.
- **Mutex Locks:** Mutex locks are software tools that provide mutual exclusion. A process must acquire the lock before entering a critical section and release the lock when exiting.

**III. Critical Section Problem**

- **Description:** The critical-section problem involves designing a protocol that allows cooperating processes to access shared resources without conflicts.
- **Requirements for a solution:**
    - **Mutual Exclusion:** Only one process can be inside the critical section at a time.
    - **Progress:** If no process is in a critical section and some processes want to enter, the selection of which process enters next cannot be delayed indefinitely.
    - **Bounded Waiting:** There must be a limit on how many times other processes can enter their critical sections before a process is granted access.
- **Kernel Code:** Operating system kernel code is also subject to race conditions when manipulating shared data structures. Kernel developers must ensure the operating system is free from these race conditions.
- **Handling Critical Sections:**
    - **Preemptive Kernels** allow processes to be preempted while running in kernel mode. This requires careful design to avoid race conditions.
    - **Non-preemptive Kernels** do not allow preemption in kernel mode. This avoids many race conditions since only one process is active in the kernel at a time.

**IV. Number of Processes**

- The number of processes that can execute concurrently is determined by the number of available CPUs or cores, whether they are physical or logical.
- On a single-processor system, processes are multiplexed on the CPU to achieve concurrency.
- On a multi-processor system, multiple processes can run in parallel.

**V. Basic Sections**

A typical process includes the following basic sections:

- **Text Section:** Contains the program code.
- **Data Section:** Contains global variables.
- **Stack:** Contains temporary data like function parameters, return addresses, and local variables.
- **Heap:** Memory that is dynamically allocated during runtime.
- A process also includes the current activity represented by the value of the **program counter** and the contents of the **processor's registers**.

**VI. Scheduler and Dispatcher**

- **Scheduler (Short-Term or CPU Scheduler):**
    - Selects a process from the ready queue for execution on the CPU.
    - The goal is to maximize CPU utilization and provide quick switching for time sharing.
    - Maintains scheduling queues of processes like job queue, ready queue and device queues.
    - The ready queue can be ordered in various ways.
- **Types of Schedulers:**
    - **Short-term scheduler (CPU scheduler)** selects from processes ready in main memory.
    - There can also be a **medium-term scheduler** which swaps processes in and out of memory.
- **Dispatcher:**
    - Gives control of the CPU to the process selected by the short-term scheduler.
    - Involves context switching, switching to user mode, and jumping to the proper location in the user program.
    - **Dispatch latency** is the time it takes for the dispatcher to stop one process and start another.
- **Responsibilities:**
    - The scheduler's responsibility is to determine which process is allocated the CPU.
    - The dispatcher is responsible for the actual mechanics of switching the CPU to that process.

**VII. Parent Process**

- A parent process is a process that creates other processes (child processes).
- The parent process can create child processes using system calls.
- The parent process may wait for the child process to finish before continuing or they might run concurrently.

**VIII. Zombie Process, Orphan Process**

- **Zombie Process:** A process that has completed execution, but its entry remains in the process table until its parent process reaps the process using the wait() system call.
- **Orphan Process:** A process whose parent has terminated before it. The operating system typically assigns the init process as the new parent of an orphan process.

**IX. Signals in Parent Processes, `fork()`, Necessity of `wait()` Signal**

- **`fork()`:** The `fork()` system call creates a new process that is a copy of the parent process. The child process inherits many attributes from the parent, such as open files and memory.
- **Signals:** Signals are used to notify a process of a particular event. The parent process can send or receive signals related to the child process’s state.
- **`wait()`:** The `wait()` system call is used by a parent process to pause its execution until one of its child processes has finished. This call reclaims the child's resources and prevents zombie processes.
- The `wait()` signal is necessary to ensure a parent is informed when a child process is complete.

Okay, let's delve into the concepts of deadlocks, multi-processors, and memory management, including the specific topics you've listed.

**I. Deadlocks**

*   **Definition:** A deadlock occurs when a set of processes are blocked because each process is holding a resource and waiting to acquire a resource held by another process. These processes can never finish because they are all waiting for each other.
*   **System Model:** A system consists of a finite number of resources (CPU cycles, memory space, I/O devices) that are distributed among competing processes. Each resource type has a certain number of instances.
*   **Conditions for Deadlock:** For a deadlock to occur, all of the following four conditions must hold simultaneously:
    *   **Mutual Exclusion:** At least one resource must be non-sharable. That is, only one process can use the resource at a time. Sharable resources, like read-only files, do not require mutual exclusion and cannot be involved in a deadlock.
    *   **Hold and Wait:** A process must hold at least one resource and be waiting to acquire additional resources that are held by other processes.
    *   **No Preemption:** Resources cannot be forcibly taken away from a process. A resource can be released only voluntarily by the process holding it, after the process has completed its task.
    *   **Circular Wait:** A set of waiting processes exists, where each process is waiting for a resource held by the next process in the chain, and the last process is waiting for a resource held by the first process. The circular wait condition implies the hold-and-wait condition, but it is useful to consider them separately.
*   **Resource Allocation Graph**: Deadlocks can be represented using a resource allocation graph. This graph shows processes, resources, and the allocation/request relationships between them.
    *   The graph has two types of vertices (nodes), processes and resources, and two types of edges. Request edges show a process requesting a resource and assignment edges show a resource allocated to a process. A cycle in this graph may indicate a deadlock.

**II. Concepts of Prevention and Avoidance**

*   **Deadlock Prevention:** Methods to ensure that at least one of the necessary conditions for deadlock cannot hold. This prevents deadlocks by limiting how requests for resources can be made.
    *   **Mutual Exclusion:** Cannot be prevented for non-sharable resources.
    *   **Hold and Wait:** Can be prevented by requiring processes to request all required resources at once or by releasing held resources before requesting new ones.
    *   **No Preemption:** Can be achieved by preempting resources from processes, which may not be feasible for all resources, such as mutex locks.
    *   **Circular Wait:** Can be prevented by imposing a total ordering of all resource types and requiring that each process requests resources in an increasing order of enumeration.
*   **Deadlock Avoidance:** Requires that the operating system be given additional information in advance about which resources a process will request and use during its lifetime. This allows the system to decide whether a request should be granted or delayed to avoid a potential deadlock.
    *   A process must declare the maximum number of resources of each type that it may need.
    *   The system dynamically examines the resource-allocation state to ensure that a circular-wait condition can never exist.
    *   A **safe state** is a state in which the system can allocate resources to processes in some order without causing a deadlock.
    *   An **unsafe state** may lead to a deadlock.
    *  **A deadlocked state is an unsafe state, but not all unsafe states are deadlocks**.

**III. Multi-processors**

*   **Functionality:**
    *   Multiprocessor systems have two or more processors in close communication, sharing the computer bus and sometimes the clock, memory, and peripheral devices.
    *   Multiprocessor systems enhance performance by allowing parallel execution of tasks.
    *   They can provide increased reliability by allowing the system to continue providing service proportional to the level of surviving hardware, known as graceful degradation. Some systems, known as fault tolerant, can survive a failure of any single component.
    *  Multiprocessor systems can use symmetric multiprocessing or asymmetric multiprocessing.
*   **Symmetric Multiprocessing (SMP):**
    *   Each processor performs all tasks within the operating system.
    *   All processors are peers; no boss-worker relationship exists between processors.
    *   Each processor has its own set of registers and a private cache, but they share physical memory.
*   **Asymmetric Multiprocessing:**
    *   Only one processor accesses the system data structures, alleviating the need for data sharing.
    *  One processor could be a master and the other the slave or they may be dedicated to certain functions.
*   **Multicore Systems:**
    *   A recent trend is to include multiple computing cores on a single chip.
    *   They are more efficient than multiple chips with single cores because on-chip communication is faster than between-chip communication, and they use less power.
    *   Multicore systems are multiprocessor systems, but not all multiprocessor systems are multicore.
    *   Multicore systems allow for parallel execution of threads, which improves concurrency and utilization of CPU resources.
    *  In multicore systems, data and task parallelism can be exploited.

**IV. Memory Management**

*   **Memory:** Main memory is central to the operation of a computer system. It is a large array of bytes, each with its own address. It is where instructions and data being used by the CPU must reside.
    *   It is the only large storage area the CPU can access directly.
*   **Logical vs. Physical Memory:**
    *   **Logical Address:** Generated by the CPU, also referred to as a virtual address. It is the address from the point of view of the process.
    *   **Physical Address:** The address seen by the memory unit that is loaded into the memory-address register of the memory. It is the actual address in the physical memory.
    *   The concept of a separate logical and physical address space is central to proper memory management. The **Memory Management Unit (MMU)** maps logical addresses to physical addresses dynamically.
*   **Memory Allocation:**
    *   **Contiguous Memory Allocation:** Each process is contained in a single contiguous section of memory.
        *   **Fixed-Size Partitions:** Memory is divided into fixed-sized partitions. Each partition may contain one process. The degree of multiprogramming is bound by the number of partitions.
        *   **Variable-Size Partitions:** The operating system keeps a table indicating which parts of memory are available and which are occupied. Memory is allocated from a hole large enough to accommodate it. When a process exits, its partition is freed, and adjacent free partitions are combined.
    *   **Non-contiguous Memory Allocation**: Techniques that permit the logical address space of a process to be noncontiguous, allowing a process to be allocated physical memory wherever such memory is available

**V. Numerical Problems**
    *   The sources do not have numerical problems. However, I can show you examples of how logical to physical address translation works using paging:
     * Assume the size of the logical address space is 2^m, the page size is 2^n, the size of the physical address space is 2^k.
      * Then logical addresses have m bits and physical addresses have k bits.
      * Each logical address can be expressed as a page number and offset within the page: page number p (m - n bits) and offset d (n bits)
      *  The page number is used as an index to the page table to find the frame number f.
      *  The physical address is a combination of the frame number and offset: frame number f (k-n bits), and offset d (n bits).
      * Here's a simple numerical example (not from the sources):  Let's say a logical address is 16 bits, a physical address is 12 bits, and page and frame sizes are 4 bits (16 bytes). Then the first 12 bits of a logical address are the page number. The page table is used to look up the frame number using this page number. For example, a logical address 0x1234 is broken down into page number (0x123) and offset (0x4). Suppose page 0x123 maps to physical frame 0x21. The physical address would be 0x214 where 0x21 is the frame and 0x4 is the offset.
*   You may want to search for additional resources for further examples.

**VI. Paging**

*   **Purpose:** Paging is a memory management scheme that allows the physical address space of a process to be non-contiguous. It avoids external fragmentation and the need for compaction.
*   **Basic Method:**
    *   Physical memory is divided into fixed-sized blocks called **frames**.
    *   Logical memory is divided into blocks of the same size called **pages**.
    *   To run a program, the program's pages are loaded into any available frames.
    *   A **page table** is used to translate logical addresses to physical addresses.
    *   The backing store is also split into pages.
    *   Each process has a page table that maps pages of the process's logical address space to physical frames.
    *  **A valid-invalid bit** can be present in a page table entry to denote if the corresponding page is in memory or not.
*   **Address Translation:**
    *   The CPU generates a logical address, which is divided into a page number and a page offset. The page number is an index into the page table. The frame number is combined with the offset to produce a physical address.
*   **Structure of Page Table**
   *   **Hierarchical Paging**:  In a large logical address space the page table itself can be very large. To avoid contiguous memory allocation for the page table, it can be divided into smaller pieces.
    * For example, in a two level paging scheme the page number is divided into two parts, and outer page number and an inner page number.
   *   **Hashed Page Tables**: A hash value is generated from the page number to look up entries in the page table.
   *   **Inverted Page Tables**: A single page table with one entry for each physical frame is created.

**VII. Purpose and Types of Fragmentation**

*   **Fragmentation** is a problem that occurs when memory is allocated and deallocated over time.
*   **External Fragmentation:** Exists when there is enough total memory space to satisfy a request but the available spaces are not contiguous. Storage is fragmented into a large number of small holes. Both first-fit and best-fit strategies for memory allocation can suffer from this issue.
*   **Internal Fragmentation:** Occurs when memory is allocated in fixed-size blocks. If a process requires less memory than the block size, the remaining space is wasted within the block. Paging can suffer from internal fragmentation because processes are not always an exact multiple of the page size.

**VIII. Mechanisms to Overcome External Fragmentation**

*   **Compaction:** Shuffling the memory contents to place all free memory together in one large block. This can be expensive and is only possible if relocation is dynamic.
*   **Paging:** Allows the logical address space of the processes to be noncontiguous, thus allowing a process to be allocated physical memory wherever such memory is available.
*   **Segmentation:** Similar to paging, segmentation allows the physical address space of a process to be noncontiguous. However, segments are of variable size and logical in nature, and are based on how users view memory, while pages are fixed-sized and physical.

Okay, let's delve into the intricacies of CPU and process management, focusing on the specific areas you've outlined: effective memory access time calculation, page tables and memory, paging and segmentation fragmentations, and contiguous vs linked memory allocation.

**I. Effective Memory Access Time (EAT) Calculation**

- **Basic Concepts**: The time it takes to access memory is crucial to overall system performance. In a system without paging, the effective access time (EAT) is simply the memory access time (ma). However, with demand paging, page faults can occur, significantly impacting EAT.
    
- **Page Faults**: A page fault occurs when a program tries to access a page that is not currently in main memory. The system has to retrieve the page from secondary storage (usually a disk), which is a very slow operation compared to accessing main memory.
    
- **EAT Formula:** The formula for calculating EAT in a demand-paging system is:
    
    - **EAT = (1 - p) * ma + p * page fault time**
    - Where 'p' is the probability of a page fault (0 ≤ p ≤ 1), and 'ma' is the memory access time.
- **Components of Page Fault Time**: Servicing a page fault involves multiple steps, including:
    
    1. Trapping to the OS.
    2. Saving the user registers and process state.
    3. Determining if the memory access is valid.
    4. Finding a free frame in memory.
    5. Reading the required page from disk into the frame.
    6. Updating the page table to reflect the page's location in memory.
    7. Restarting the interrupted instruction.
    
    - The disk read is the most time-consuming step, often taking several milliseconds.
- **Example Calculation**: Let's take an example:
    
    - Memory access time (ma) = 200 nanoseconds
    - Average page-fault service time = 8 milliseconds (8,000,000 nanoseconds).
    - EAT = (1 - p) * 200 + p * 8,000,000 = 200 + 7,999,800 * p
    - If the page fault rate (p) is 1/1000 (0.001), then EAT = 200 + 7,999,800 * 0.001 = 8.2 microseconds, a significant slowdown.
    - To keep the slowdown below 10%, the page-fault probability should be less than 0.0000025.
- **Translation Look-Aside Buffer (TLB)**: To improve performance, a special hardware cache called the Translation Look-Aside Buffer (TLB) is used. The TLB stores frequently accessed page table entries, allowing fast address translation.
    
- **TLB Hit Ratio**: The **hit ratio** (α) is the percentage of times the page number is found in the TLB.
    
    - If the page number is found in the TLB (a TLB hit), the memory access is very fast.
    - If the page number is not found in the TLB (a TLB miss), the system must access the page table in main memory, which is much slower.
    - EAT with TLB can be calculated as: **EAT = (1 + ε) α + (2 + ε)(1 – α) = 2 + ε – α**, where ε is the TLB search time. For instance, if α = 80%, ε = 20ns, and memory access time = 100ns, then EAT is 120 ns. A more realistic hit ratio of 99% results in an EAT of 101ns.

**II. Page Table and Page Memory**

- **Page Table Basics**:
    
    - Paging divides the logical address space of a process into fixed-size blocks called **pages** and the physical memory into blocks of the same size called **frames**.
    - A page table is used to translate the logical addresses used by the process into physical addresses in memory. Each entry in the page table corresponds to a page of the process.
    - The page table stores the **base address of each page** in physical memory.
    - Each entry may also contain a **valid-invalid bit** to indicate whether the corresponding page is currently in memory or not.
    - Page table entries often have protection bits to enforce access restrictions.
- **Logical vs Physical Addresses**:
    
    - A **logical address** is generated by the CPU, also referred to as a virtual address.
    - A **physical address** is the address seen by the memory unit, the one loaded into the memory-address register of the memory.
    - The page table performs the mapping of the logical address to the physical address.
- **Address Translation**: A logical address is divided into two parts:
    
    - **Page number (p)**: Used as an index into the page table to find the corresponding frame number in physical memory.
    - **Page offset (d)**: Indicates the position within the page.
    - The frame number from the page table is combined with the page offset to create the physical address.
- **Page Size**: Page size is a power of 2, varying between 512 bytes and 1 GB. A typical size is between 4KB and 8KB. The page size is defined by hardware.
    
    - **Calculating the number of pages/frames**: If the logical address space is 2^m and page size is 2^n bytes, then the high-order m-n bits designate the page number and the low-order n bits designate the page offset.
    - For example, if a system has a 32-bit logical address space and a 4KB (2^12) page size, then there are 20 bits for page number and 12 bits for the page offset.
- **Page Table Storage**:
    
    - Page tables are kept in main memory.
    - A **page-table base register (PTBR)** points to the start of the page table in memory.
    - A **page table length register (PTLR)** indicates the size of the page table.
    - Each entry in the page table typically takes 4 bytes.
    - For a 32-bit system, each page table entry can point to 2^32 page frames. If each frame is 4KB, the system can address 16 TB of physical memory.
- **Hierarchical Paging**: In systems with large logical address spaces, page tables can become very large and need to be paged themselves.
    
    - A two-level paging scheme involves paging the page table itself, dividing the logical address into an outer page number, an inner page number, and an offset.
    - For 64 bit address spaces, a two-level paging is not appropriate and three or more levels are needed to manage the large page tables.
- **Inverted Page Table**:
    
    - An inverted page table has one entry for each physical frame rather than one for each logical page.
    - Each entry is in the form of <process-id, page-number>, where the process-id acts as the address-space identifier.
    - The inverted page table is searched by the operating system to find a match and then the physical address <i, offset> is generated.
    - To alleviate the time for searching, a hash table is often used to limit the search.

**III. Paging and Segmentation Fragmentation**

- **Fragmentation**: Fragmentation is a problem in memory management where memory space is wasted, and it can occur in multiple forms.
    
    - **Internal Fragmentation**: Occurs when memory allocated to a process is slightly larger than the actual memory required by the process. This is common in paging as a process is given a whole page even if its data does not fully occupy the space. The wasted space is inside the allocated partition.
        - For example, if a process needs 72,766 bytes and the page size is 2048 bytes, the process needs 35 pages + 1086 bytes. The internal fragmentation is 2048 - 1086 = 962 bytes. In the worst case, internal fragmentation can be almost a full frame (1 frame - 1 byte). On average fragmentation = 1/2 of a frame size.
    - **External Fragmentation**: Occurs when there is enough total free memory to satisfy a request, but the available memory is not contiguous. This is common in dynamic memory allocation such as in segmentation.
- **Paging and Fragmentation**:
    
    - **Paging avoids external fragmentation**. Since memory is allocated in fixed-size frames, there is no issue of fitting variable-size chunks in memory.
    - **Paging suffers from internal fragmentation** due to the fixed page size.
        - Using smaller pages can reduce internal fragmentation, but it increases overhead due to more page table entries and less efficient disk I/O.
- **Segmentation and Fragmentation**
    
    - **Segmentation avoids internal fragmentation** because each segment can be sized to exactly fit the data structure or code it contains.
    - **Segmentation can suffer from external fragmentation** because segments are of different sizes, and as segments are loaded and unloaded from memory, holes of various sizes will be created. Compaction (relocating segments) may be necessary to address this.

**IV. Contiguous vs. Linked Memory Allocation**

- **Contiguous Memory Allocation**:
    
    - In contiguous memory allocation, each process occupies a single, contiguous block of memory.
    - It involves allocating a single block of memory, which can be of a fixed or variable size.
    - The entire process is stored in that block of memory.
    - **Fixed-size partitioning** was an early form of memory allocation where memory is divided into fixed-size partitions. Each partition could hold exactly one process, limiting the degree of multiprogramming.
    - **Variable-size partitioning** allows partitions to be sized to a given process's needs, thus being more efficient, though it does create the potential for external fragmentation.
- **Linked Memory Allocation**:
    
    - **Paging** and **Segmentation** are techniques that allow the logical address space of the processes to be noncontiguous, thus allowing a process to be allocated physical memory wherever such memory is available.
    - In **paging**, the process's memory is broken into pages and is allocated to non-contiguous frames in the physical memory.
    - In **segmentation**, a program is a collection of segments, each representing a logical unit and these segments can be placed in non-contiguous parts of memory.
- **Comparison**
    
    - Contiguous memory allocation is simpler to implement but is susceptible to external fragmentation with variable partitions.
    - Linked allocation, such as paging, allows for non-contiguous memory allocation and avoids external fragmentation but has the overhead of address translation via page tables.

Okay, let's explore **system calls and the kernel**, focusing on their functionalities, purpose, and the distinctions between kernel and user operations, including the costs associated with each.

**I. System Calls**

*   **Fundamental Functionalities:**
    *   System calls provide an **interface** for user programs to request services from the operating system.
    *   These services are reserved for the operating system, often involving privileged operations.
    *   They act as a bridge between the user-level applications and the kernel-level functionalities.
    *   System calls are usually available as routines written in C and C++.
    *   Low-level tasks, where direct hardware access is needed, may require assembly language.
    *   A system call is typically treated as a software interrupt by the hardware.

*   **Why They Are Needed:**
    *   **Protection:** User programs cannot directly access hardware or execute privileged instructions. System calls provide a controlled way to access these resources.
    *   **Resource Management**: The OS manages system resources like memory, I/O devices, and files. System calls enable user programs to request these resources in a controlled way.
    *   **Abstraction**: System calls provide a level of abstraction, hiding the complexities of hardware interaction from user programs.
    *   **Efficiency:** The OS is better equipped to handle resource management, leading to more efficient system operation.

*   **Examples of System Call Usage**:
    *   **I/O operations:** Reading from or writing to a file or device.
    *   **Program Execution:** Loading and executing a new program.
    *   **Process Management**: Creating, terminating, or managing processes.
    *   **File Management:** Creating, deleting, opening, closing, reading, and writing files.
    *   **Device Management**: Requesting and releasing devices, reading and writing from/to them.
    *   **Information Maintenance**: Getting the time and date, system data, process attributes.
    *    **Communication**: Creating connections and sending or receiving messages with other processes.
    *   **Protection:** Setting permissions and controlling access to resources.
*   **How System Calls Work:**
    1.  A user program requests a service by invoking a system call via an API.
    2.  The system call is implemented as a trap to a specific location in the interrupt vector.
    3.  The hardware treats the system call as a software interrupt.
    4.  Control is passed to a service routine within the OS kernel, and the mode bit switches to kernel mode.
    5.  The kernel identifies the system call and performs the requested operation.
    6.  After execution, the kernel returns control to the user program, and the mode bit switches back to user mode.
    7.  Parameters for the system call can be passed in registers, on the stack, or in memory.
    8.  A system call interface maintains a table indexed according to the system call numbers.

*   **System Calls and APIs**
    *   Application developers typically use an **Application Programming Interface (API)**, which is a set of functions that are available to programmers.
    *   The API hides the details of the underlying system calls.
    *   The API is accessed through a library of code provided by the operating system.
    *   For example, the C standard library (`libc`) is used for programs written in the C language.
    *   There is often a strong correlation between API functions and the native system calls provided by the OS.

*   **Generic vs. Specific System Calls:**
    *   System call names used in the text are often generic examples, and each OS has its specific names.
    *   For example, `CreateProcess()` in Windows corresponds to `fork()` in UNIX.

**II. Monitor Calls**

*   **Purpose and Significance**: The term "monitor call" is another name for a system call. Therefore, everything described under system calls applies to monitor calls, emphasizing that a system call is a request for the operating system's monitor to perform a function.
    *   The term "monitor" in this context refers to the operating system's kernel or supervisor, responsible for managing system resources.
    *   A monitor call is essentially a way to transition from user mode to kernel mode to execute a privileged function, and it is typically used in systems that provide a protected environment to avoid direct hardware access from user processes.

**III. Kernel**

*   **Functionality and Purpose:**
    *   The kernel is the **core** of the operating system.
    *  It is the program that is always running on the computer.
    *   It **manages** the computer's hardware and software resources.
    *   It acts as an intermediary between the user and the hardware.
    *   The kernel is responsible for fundamental tasks such as:
        *   **Process management:** Scheduling processes, creating/deleting processes and threads, synchronization.
        *   **Memory management:** Allocating and deallocating memory, managing virtual memory.
        *   **I/O management:** Controlling I/O devices, managing device drivers, interrupt handling.
        *   **File system management:** Managing files and directories.
        *   **Security:** Enforcing protection mechanisms and security policies.
    *   The kernel is loaded into memory during the **booting** process.
    *   It typically resides in a protected area of memory.
    *   The kernel remains in memory during the entire computer session.
    *   The kernel may be made of various modules which can be loaded while the operating system is running.

*   **Kernel Structure:**
    *   **Monolithic Kernel**: In this structure, all kernel services run in the kernel address space. It is efficient but can be harder to maintain. Examples are UNIX, Linux, and Windows.
    *   **Microkernel**:  This structure has a small core kernel providing essential services, with other services running in user space as user processes. It is easier to maintain and more secure but can have performance overhead. Examples are Mach and QNX.
    *   **Modular Kernel**: This approach combines features of monolithic and microkernels, with a core kernel and dynamically loadable modules, which is used by many modern OS's, including Linux and Solaris.
    *   **Layered Kernel**: This is an approach where the OS is broken into layers, each with defined operations that can be invoked by higher-level layers.

**IV. Differences Between Kernel and User Operations**

*   **Mode of Operation**:
    *   The CPU operates in **two modes**: user mode and kernel mode.
    *   A **mode bit** in the hardware indicates the current mode: 0 for kernel mode, 1 for user mode.
    *   When a user application runs, the system is in user mode.
    *  When the operating system runs, the system is in kernel mode.
    *   Kernel mode is also known as supervisor mode, system mode, or privileged mode.
    *   At boot time, the hardware starts in kernel mode.
    *   The system switches to kernel mode upon a trap, an interrupt, or a system call.
    *   Before passing control to a user program, the system switches to user mode.
*   **Privileged Instructions**:
    *   Certain instructions, called **privileged instructions**, can only be executed in kernel mode.
    *   Examples include I/O control, timer management, and interrupt management.
    *   If a user program attempts to execute a privileged instruction, it is treated as an illegal operation, and the hardware traps it to the OS.

* **Access to Memory**
    *  In kernel mode, the operating system has unrestricted access to both system memory and user memory. This is necessary so that the OS can load user programs, access and modify system call parameters, perform I/O to and from user memory, perform context switches, etc.
    * In user mode, the operating system restricts memory access to the user's own memory space. User mode programs cannot directly access kernel data structures or memory locations that belong to other processes.

**V. Kernel-Level vs. User-Level Processes/Threads**

*   **Kernel-Level Threads**:
    *   Kernel-level threads are managed directly by the operating system's kernel.
    *   The kernel is aware of these threads and schedules them to run on the CPU.
    *   Context switching between kernel-level threads is faster.
    *   Kernel-supported threads can be scheduled independently.
    *   Examples are threads in Linux, Windows, and Solaris.
*   **User-Level Threads**:
    *   User-level threads are managed by a thread library in user space, without kernel support.
    *   The kernel is not aware of user-level threads.
    *   For user-level threads, if one thread makes a system call, the entire process can be blocked.
    *   User-level threads are transparent to the kernel.
    *   Thread management in user space is more efficient, but it cannot take advantage of multiple cores.
    *   Examples of systems that support user-level threads are Java (early versions), and Solaris (early versions).
    *   User-level threads do not have direct access to kernel resources.
    *   Thread can be created in both user and kernel mode, but processes are created only in kernel mode.

**VI. Cost: User vs. Kernel Processes**

*   **User Processes/Threads**
    *   Less overhead for creation and management.
    *   Faster context switching within the same process since the OS is not involved.
    *   User-level threads are limited by the scheduling and resource constraints of the parent process and by being unable to take advantage of multiple cores.
*   **Kernel Processes/Threads**
    *   More overhead for creation and management.
    *   Slower context switching because the OS is involved in saving and restoring process/thread states.
    *   Full access to system resources and the ability to leverage multicore processing.
    *   System calls incur a higher cost compared to user-space calls, as the system needs to switch modes, save and restore state, etc.

Okay, let's delve into **Disk Management**, focusing on **Disk Scheduling**, its concepts, problems, and different types, drawing from the sources provided.

**I. Disk Scheduling: Concepts and Problems**

- **Objective:** The primary goal of disk scheduling is to efficiently manage disk I/O requests to optimize performance. This involves minimizing the time it takes to access data on a disk and maximizing the disk's bandwidth.
    
- **Disk Access Time:** The time it takes to access data on a disk is composed of two main parts:
    
    - **Seek Time**: The time it takes for the disk arm to move the read/write heads to the correct cylinder.
    - **Rotational Latency**: The time it takes for the desired sector to rotate under the read/write head.
    - Additionally, there is a **transfer time**, the time it takes to transfer data between the disk and the computer.
- **Disk Bandwidth:** The total number of bytes transferred, divided by the total time between the first request for service and the completion of the last transfer.
    
- **I/O Request Information**: When a process needs to perform disk I/O, it issues a system call to the OS. The request includes information such as:
    
    - Whether the operation is input or output.
    - The disk address for the transfer.
    - The memory address for the transfer.
    - The number of sectors to be transferred.
- **Disk Queue:** In a multiprogramming environment, the disk queue often has multiple pending requests. When a request is completed, the OS must choose which pending request to service next. Disk scheduling algorithms are used for this selection.
    
- **Need for Scheduling:** Disk scheduling is essential because the time it takes to access data on a disk is significantly affected by the order in which the requests are serviced. Efficient scheduling minimizes seek times and rotational latencies, leading to faster data access and improved overall system performance.
    
- **Factors Affecting Disk Scheduling**:
    
    - The file allocation method can greatly influence disk service requests. For example, a contiguously allocated file will generate requests that are close together on the disk.
    - The location of directories and index blocks is important because they are frequently accessed.
    - The number and types of I/O requests influence the performance of any disk-scheduling algorithm.
- **Problems with Disk Scheduling**
    
    - **Starvation**: Some scheduling algorithms may lead to certain requests being delayed indefinitely.
    - **Fairness**: Some algorithms might favor certain requests, resulting in unfair service for other requests.
    - **Optimality**: It is difficult to find an optimal schedule as the computation needed to find it may not justify the small performance gains over simpler algorithms.
    - **Rotational Latency**: Modern disks do not disclose the physical location of logical blocks, making it difficult for the OS to schedule for improved rotational latency.
    - **Conflicting Goals**: The OS may have other constraints on the service order for requests, such as giving priority to demand paging over application I/O, and this may conflict with the optimization of disk performance.
    - **SSD Specifics**: Disk scheduling algorithms designed for magnetic disks don't apply to Solid State Drives (SSDs) since SSDs do not have moving parts. SSDs typically use a simple FCFS policy.

**II. Types of Disk Scheduling Algorithms**

The sources discuss several disk-scheduling algorithms, including:

1. **First-Come, First-Served (FCFS)**:
    
    - **Concept**: Requests are serviced in the order they arrive.
    - **Fairness**: It is intrinsically fair.
    - **Performance**: Generally does not provide the fastest service.
    - **Example**: Given a disk queue with requests for cylinders 98, 183, 37, 122, 14, 124, 65, and 67, and the disk head starting at cylinder 53, FCFS would result in a total head movement of 640 cylinders. This is shown in Figure 10.4.
    - **Implementation:** Simple to implement.
    - **Limitations**: Can result in large seek times and does not optimize disk head movement.
2. **Shortest-Seek-Time-First (SSTF)**:
    
    - **Concept**: The request with the minimum seek time from the current head position is selected next.
    - **Performance**: Generally provides better performance than FCFS, by reducing total head movement.
    - **Starvation**: Can cause starvation for some requests. Requests far from the current head position may be repeatedly postponed.
    - **Example**: Given the same disk queue (98, 183, 37, 122, 14, 124, 65, 67) and head starting at 53, SSTF would result in a total head movement of 236 cylinders.
    - **Implementation:** A form of Shortest-Job-First (SJF) scheduling.
    - **Limitations**: Can lead to starvation and is not necessarily optimal.
3. **SCAN (Elevator Algorithm)**:
    
    - **Concept**: The disk arm starts at one end of the disk and moves towards the other end, servicing requests along the way. When it reaches the end, the direction is reversed.
    - **Fairness:** Provides a more uniform wait time than SSTF.
    - **Performance:** Performs better for systems that place a heavy load on the disk, with reduced risk of starvation.
    - **Example**: If the disk arm is moving towards 0 and the initial head position is 53, SCAN would service requests at 37 and 14. At cylinder 0, the arm reverses and moves to the other end of the disk, servicing requests at 65, 67, 98, 122, 124, and 183.
    - **Implementation:** Slightly more complex than SSTF.
    - **Limitations**: Can result in longer wait times for requests at the other end of the disk when the head is moving in the opposite direction.
4. **C-SCAN (Circular SCAN)**:
    
    - **Concept**: Similar to SCAN, but after reaching the end of the disk, the head immediately returns to the beginning without servicing any requests on the return trip. Then the scan begins again.
    - **Fairness**: Provides a more uniform wait time than SCAN because the head only services requests in one direction.
    - **Performance**: More uniform wait times compared to SCAN.
    - **Implementation:** Slightly more complex than SCAN.
    - **Limitations**: Can increase the average wait time compared to SCAN.
5. **LOOK**
    
    - **Concept**: Similar to SCAN, but the disk arm only goes as far as the final request in each direction. It doesn't need to travel to the very edge of the disk if there is no request to be serviced.
    - **Performance**: Reduces unnecessary travel compared to SCAN.
    - **Implementation**: More efficient than SCAN in certain scenarios where requests are not distributed across the full range of cylinders.
6. **C-LOOK**
    
    - **Concept**: Similar to C-SCAN but the disk arm only goes as far as the final request in each direction, similar to LOOK.
    - **Fairness**: Provides a more uniform wait time compared to LOOK.
    - **Performance**: More efficient than C-SCAN because it eliminates unnecessary travel.
    - **Implementation**: More complex than SCAN and LOOK.

**III. Other Considerations**

- **Disk Controller Scheduling:** Modern disk controllers often have their own built-in caches and scheduling capabilities, which can further optimize disk I/O operations. The OS may send a batch of requests to the controller, and the controller can schedule them to improve both seek time and rotational latency.
- **Operating System Control:** While disk controllers have some scheduling capabilities, the operating system may still choose to do its own disk scheduling to meet specific requirements such as giving priority to demand paging.
- **Disk Scheduling and SSDs**: The disk scheduling algorithms are mainly designed for magnetic disk drives. For SSDs, a simple FCFS algorithm is commonly used, sometimes with modifications to merge adjacent requests.

In summary, disk scheduling is a crucial aspect of operating system functionality for optimizing the performance of disk I/O operations. Different scheduling algorithms offer various trade-offs in terms of performance, fairness, and implementation complexity, and selecting the right algorithm depends on the specific needs and characteristics of the system. Let me know if you would like to explore any of these topics further.


*References





